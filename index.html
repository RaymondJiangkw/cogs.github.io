<!DOCTYPE html>
<html>
    <head lang="en">
        <meta charset="UTF-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <title>COGS</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
        <link rel="stylesheet" href="style.css">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    </head>
    <body>
        <div class="container" id="main">
            <div class="row">
                <h2 class="col-md-12 text-center">
                    <b>A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose</b><br>
                    <small style="color:black;">
                    ACM SIGGRAPH 2024
                    </small>
                </h2>
            </div>
            <div class="row">
                <div class="col-md-12 text-center">
                    <ul class="list-inline">
                        <li>
                            <a href="https://raymondjiangkw.github.io/">
                            Kaiwen Jiang
                            </a>
                        </li>
                        <li>
                            <a href="https://oasisyang.github.io/">
                            Yang Fu
                            </a>
                        </li>
                        <li>
                            <a href="https://mukundvarmat.github.io/">
                            Mukund Varma T
                            </a>
                        </li>
                        <li>
                            <a href="https://yashbelhe.github.io/">
                            Yash Belhe
                            </a>
                        </li>
                        <li>
                            <a href="https://xiaolonw.github.io/">
                            Xiaolong Wang
                            </a>
                        </li>
                        <li>
                            <a href="https://cseweb.ucsd.edu/~haosu/">
                            Hao Su
                            </a>
                        </li>
                        <li>
                            <a href="https://cseweb.ucsd.edu/~ravir/">
                            Ravi Ramamoorthi
                            </a>
                        </li>
                        <br>
                        <li>
                            University of California at San Diego
                        </li>
                    </ul>
                </div>
            </div>
            <div class="row">
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="list-inline">
                        <li>
                            <a href="https://arxiv.org/abs/2405.03659">
                                <image src="assets/pdf.png" height="80px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://cseweb.ucsd.edu/~ravir/kaiwensig_supp.pdf">
                                <image src="assets/supplementary.png" height="80px">
                                <h4><strong>Supplementary</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=0wqQnHD1R6Q">
                                <image src="assets/youtube_icon.png" height="80px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/RaymondJiangkw/COGS">
                                <image src="assets/github.png" height="80px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <!-- Slideshow container -->
                    <div class="slideshow-container">
                        <!-- Full-width images with number and caption text -->
                        <div class="mySlides">
                            <div class="numbertext">1 / 3</div>
                            <img src="assets/12_views.gif" style="width:100%">
                            <div class="text">12 Views</div>
                        </div>
                        <div class="mySlides">
                            <div class="numbertext">2 / 3</div>
                            <img src="assets/6_views.gif" style="width:100%">
                            <div class="text">6 Views</div>
                        </div>
                        <div class="mySlides">
                            <div class="numbertext">3 / 3</div>
                            <img src="assets/3_views.gif" style="width:100%">
                            <div class="text">3 Views</div>
                        </div>
                        <!-- Next and previous buttons -->
                        <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
                        <a class="next" onclick="plusSlides(1)">&#10095;</a>
                    </div>
                    <br>
                    <!-- The dots/circles -->
                    <div style="text-align:center">
                        <span class="dot" onclick="currentSlide(1)"></span>
                        <span class="dot" onclick="currentSlide(2)"></span>
                        <span class="dot" onclick="currentSlide(3)"></span>
                    </div>
                </div>
            </div>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <center>
                        <h3><b>Abstract</b>
                        </h3>
                    </center>
                    <p class="text-justify">
                        <i>Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we <i>construct</i> a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we <i>optimize</i> the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset.</i>
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <center>
                        <h3><b>Video</b>
                        </h3>
                        <iframe width="640" height="360" src="https://www.youtube.com/embed/0wqQnHD1R6Q"></iframe>
                    </center>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <center>
                        <h3><b>Overview</b>
                        </h3>
                    </center>
                    <center>
                        <image src="assets/pipeline1.png" class="img-responsive" alt="overview" width="100%" style="max-height: 450px;margin:auto;">
                    </center>
                    <p>
                        Overview of our method for sparse view synthesis. 
                        We first back-project the first view and sequentially register, adjust and back-project the remaining views in sequence to obtain a coarse solution.
                        This coarse solution is then refined by standard optimization to reproduce fine details.
                    </p>
                    <br>
                    <center>
                        <image src="assets/pipeline2.png" class="img-responsive" alt="overview" width="100%" style="max-height: 450px;margin:auto;">
                    </center>
                    <p>We assume the first <span class="math inline"><em>k</em></span> views
                        have already been registered, and illustrate the registration,
                        adjustment and back-projection of the <span
                            class="math inline"><em>k</em> + 1</span> view. We first initialize
                        the camera pose of the <span class="math inline"><em>k</em> + 1</span>
                        view, denoted as <span
                            class="math inline"><em>P</em><sub><em>k</em> + 1</sub></span>, as the
                        <span class="math inline"><em>k</em></span> view’s camera pose. 2D
                        correspondences are detected between ground-truth image <span
                            class="math inline"><em>I</em><sub><em>k</em> + 1</sub></span> and the
                        rendered result <span
                            class="math inline"><em>I</em><sub>render</sub>(<em>P</em><sub><em>k</em> + 1</sub>)</span>
                        at <span class="math inline"><em>P</em><sub><em>k</em> + 1</sub></span>.
                        Correspondence points on <span
                            class="math inline"><em>I</em><sub>render</sub>(<em>P</em><sub><em>k</em> + 1</sub>)</span>
                        are denoted as <span class="math inline"><em>κ</em>′</span>, while those
                        on <span class="math inline"><em>I</em><sub><em>k</em> + 1</sub></span>
                        are denoted as <span class="math inline"><em>κ</em></span>. Green points
                        denote correct correspondences, while red points denote wrong
                        correspondences. We can use perspective-n-points (PnP) to solve the
                        camera pose but it results in an erroneous solution. We then apply our
                        optimization pipeline to estimate the camera pose for
                        registration. For now, the monocular depth <span
                            class="math inline"><em>D</em><sub><em>k</em> + 1</sub></span> of the
                        <span class="math inline"><em>k</em> + 1</span> view deviates
                        significantly from the rendered depth <span
                            class="math inline"><em>D</em><sub>render</sub>(<em>P</em><sub><em>k</em> + 1</sub>)</span>
                        at <span class="math inline"><em>P</em><sub><em>k</em> + 1</sub></span>.
                        Afterwards, we apply our optimization pipeline to adjust all previous
                        registered camera poses and monocular depths along with <span
                            class="math inline"><em>P</em><sub><em>k</em> + 1</sub></span> and <span
                            class="math inline"><em>D</em><sub><em>k</em> + 1</sub></span>. It can
                        be seen that <span
                            class="math inline"><em>I</em><sub>render</sub>(<em>P</em><sub><em>k</em> + 1</sub>)</span>
                        and <span class="math inline"><em>D</em><sub><em>k</em> + 1</sub></span>
                        are much close to <span
                            class="math inline"><em>I</em><sub><em>k</em> + 1</sub></span> and <span
                            class="math inline"><em>D</em><sub>render</sub>(<em>P</em><sub><em>k</em> + 1</sub>)</span>.
                        Finally, we back-project pixels in the <span
                            class="math inline"><em>k</em> + 1</span> view into world space as 3D
                        Gaussians based on <span
                            class="math inline"><em>D</em><sub><em>k</em> + 1</sub></span>.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <center>
                        <h3><b>Results</b>
                        </h3>
                    </center>
                    <center>
                        <image src="assets/quantitative_results.png" width="100%">
                    </center>
                    <center>
                        <p>
                            We show quantitative comparison against pose dependent and independent techiques (indicated appropriately) for sparse view synthesis (3, 6 and 12 input training views, all other views are used for testing). 
                        </p>
                    </center>
                    <br>
                    <center>
                        <image src="assets/results.jpg" width="100%">
                    </center>
                    <center>
                        <p>
                            We illustrate qualitative comparison against pose dependent and independent techiques (indicated appropriately) for sparse view synthesis (3, 6 and 12 input training views). 
                        </p>
                    </center>
                </div>
            </div>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3><b>Citation</b>
                    </h3>
                    <pre>
@article{COGS2024,
    title={A Construct-Optimize Approach to Sparse View Synthesis without Camera Pose},
    author={Jiang, Kaiwen and Fu, Yang and Varma T, Mukund and Belhe, Yash and Wang, Xiaolong and Su, Hao and Ramamoorthi, Ravi},
    journal={SIGGRAPH},
    year={2024}
}</pre>
                </div>
            </div>
        </div>
    </body>
    <script src="script.js"></script>
    <footer>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-center">
                    This website is adapted from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </footer>
</html>